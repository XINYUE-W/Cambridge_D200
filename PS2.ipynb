{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503c5978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from choice_learn.datasets import load_modecanada\n",
    "\n",
    "transport_df = load_modecanada(as_frame=True)\n",
    "print(f\"Dataset shape: {transport_df.shape}\")\n",
    "display(transport_df.head(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514ff12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1a\n",
    "case1 = transport_df[transport_df[\"case\"] == 1]\n",
    "print(\"\\nCase 1:\")\n",
    "print(case1[[\"case\", \"alt\", \"choice\"]])\n",
    "print(f\"Number of alternatives available: {len(case1)}\")\n",
    "print(f\"Chosen alternative: {case1[case1['choice'] == 1]['alt'].values[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c116804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1b\n",
    "\n",
    "from choice_learn.data import ChoiceDataset\n",
    "\n",
    "canada_dataset = ChoiceDataset.from_single_long_df(\n",
    "    df=transport_df,\n",
    "    items_id_column=\"alt\",\n",
    "    choices_id_column=\"case\",\n",
    "    choices_column=\"choice\",\n",
    "    shared_features_columns=[\"income\"],\n",
    "    items_features_columns=[\"cost\", \"freq\", \"ovt\", \"ivt\"],\n",
    "    choice_format=\"one_zero\"\n",
    ")\n",
    "print(canada_dataset.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce2b0ae",
   "metadata": {},
   "source": [
    "Question 1c\n",
    "In-vehicle time (ivt) has ALTERNATIVE-SPECIFIC coefficients because the subjective experience of time differs. Obviously, on a plane people can sleep or watch movies but in a car you must focus on driving and suffer from the tiredness. Moreover, the flexibility level on a train is more than a car but less than a plane. \n",
    "Therefore, a single shared coefficient would force the same disutility across different mode which does not make any sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d62594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1d\n",
    "\n",
    "from choice_learn.models import ConditionalLogit\n",
    "\n",
    "model = ConditionalLogit()\n",
    "\n",
    "# Shared coefficients — one β for all modes\n",
    "# The assumption is that the effect of cost (or freq, ovt) on utility\n",
    "# doesn't depend on which mode you're looking at\n",
    "model.add_shared_coefficient(coefficient_name=\"beta_cost\", feature_name=\"cost\", items_indexes=[0, 1, 2, 3])\n",
    "model.add_shared_coefficient(coefficient_name=\"beta_freq\", feature_name=\"freq\", items_indexes=[0, 1, 2, 3])\n",
    "model.add_shared_coefficient(coefficient_name=\"beta_ovt\",  feature_name=\"ovt\",  items_indexes=[0, 1, 2, 3])\n",
    "\n",
    "# Alternative-specific ivt — each mode has its own coefficient\n",
    "# because the subjective experience of time varies by mode\n",
    "model.add_coefficients(coefficient_name=\"beta_ivt\",    feature_name=\"ivt\",       items_indexes=[0, 1, 2, 3])\n",
    "\n",
    "# Alternative-specific intercepts and income effects\n",
    "# Car (index 2) is the reference category, so it's excluded here\n",
    "# All other coefficients are interpreted relative to car\n",
    "model.add_coefficients(coefficient_name=\"beta_inter\",  feature_name=\"intercept\", items_indexes=[0, 1, 3])\n",
    "model.add_coefficients(coefficient_name=\"beta_income\", feature_name=\"income\",    items_indexes=[0, 1, 3])\n",
    "\n",
    "# Fit the model — lbfgs is the default optimizer for ConditionalLogit\n",
    "history = model.fit(canada_dataset)\n",
    "report  = model.compute_report(canada_dataset)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bcb098",
   "metadata": {},
   "source": [
    "Question 1e\n",
    "\n",
    "1. Sign of β_cost:\n",
    "   We expect β_cost < 0. Higher cost reduces utility, so travelers should be\n",
    "   less likely to choose an expensive mode. A negative coefficient is consistent\n",
    "   with basic demand theory — if it came out positive, that would suggest a\n",
    "   model misspecification or severe multicollinearity.\n",
    "\n",
    "2. Intercepts across modes:\n",
    "   The intercept β^inter_j captures the baseline desirability of mode j\n",
    "   relative to car (the reference), after controlling for cost, time, and frequency.\n",
    "   The mode with the highest intercept is the one people \"default to\" when all\n",
    "   observable attributes are equal. In ModeCanada, car typically serves as the\n",
    "   reference (β^inter_car = 0 by construction), and the other modes often show\n",
    "   negative intercepts — meaning car is generally preferred as a baseline.\n",
    "\n",
    "3. Income coefficients:\n",
    "   β^income_j tells us how income shifts relative preference toward mode j vs. car.\n",
    "   - Positive β^income_air → higher income people are more likely to fly (air is a luxury good)\n",
    "   - Negative β^income_bus → higher income people avoid buses (bus is an inferior good)\n",
    "   - β^income_train depends on the route; train sometimes competes with air for wealthier travelers\n",
    "   This reveals a clear income gradient in transportation mode choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a7d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1f\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "car_mean_cost = transport_df.loc[transport_df[\"alt\"] == \"car\", \"cost\"].mean()\n",
    "print(f\"Mean car cost = {car_mean_cost:.2f}\")\n",
    "\n",
    "\n",
    "probs = np.asarray(model.predict_probas(canada_dataset))\n",
    "print(\"probs shape:\", probs.shape)\n",
    "\n",
    "if hasattr(canada_dataset, \"items_ids\"):\n",
    "    items = list(canada_dataset.items_ids)\n",
    "elif hasattr(canada_dataset, \"items\"):\n",
    "    items = list(canada_dataset.items)\n",
    "else:\n",
    "    \n",
    "    items = sorted(transport_df[\"alt\"].unique().tolist())\n",
    "\n",
    "car_index = items.index(\"car\")\n",
    "print(\"Items order:\", items)\n",
    "print(\"car_index =\", car_index)\n",
    "\n",
    "car_mean_prob = probs[:, car_index].mean()\n",
    "print(f\"Mean P(car) = {car_mean_prob:.4f}\")\n",
    "\n",
    "\n",
    "beta_cost = None\n",
    "for w in model.trainable_weights:\n",
    "    if \"beta_cost\" in w.name:\n",
    "        beta_cost = float(w.numpy().squeeze())\n",
    "        break\n",
    "\n",
    "if beta_cost is None:\n",
    "    raise ValueError(\"Can't find beta_cost in model.trainable_weights. Print weights' names to check.\")\n",
    "\n",
    "print(f\"beta_cost = {beta_cost:.6f}\")\n",
    "\n",
    "\n",
    "eta_car = beta_cost * car_mean_cost * (1 - car_mean_prob)\n",
    "print(f\"Own-price elasticity (car) = {eta_car:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db00f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2a\n",
    "# I cannot use the preprocessing=\"rumnet\" as requested since there are errors showing \"all input arrays must have the same shape\"\n",
    "from choice_learn.datasets import load_expedia\n",
    "expedia_df = load_expedia(as_frame=True)\n",
    "\n",
    "#take first 5000 as requested\n",
    "top_ids = expedia_df[\"srch_id\"].unique()[:5000]\n",
    "expedia_df = expedia_df[expedia_df[\"srch_id\"].isin(top_ids)].copy()\n",
    "\n",
    "expedia_df = expedia_df.fillna(0)\n",
    "\n",
    "print(f\"Rows: {len(expedia_df)}\")\n",
    "print(f\"Unique searches (choices): {expedia_df['srch_id'].nunique()}\")\n",
    "print(f\"Columns: {expedia_df.shape[1]}\")\n",
    "\n",
    "\n",
    "sizes = expedia_df.groupby(\"srch_id\").size()\n",
    "print(f\"\\nChoice set size — mean: {sizes.mean():.1f}, min: {sizes.min()}, max: {sizes.max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69bedc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2b\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd         \n",
    "item_features = [\"prop_starrating\", \"prop_review_score\", \"prop_brand_bool\",\n",
    "                 \"prop_location_score1\", \"promotion_flag\"]\n",
    "\n",
    "# log(price)\n",
    "expedia_df[\"log_price\"] = np.log1p(expedia_df[\"price_usd\"])\n",
    "item_features = [\"log_price\"] + item_features\n",
    "\n",
    "# ChoiceDataset\n",
    "# \n",
    "MAX_ITEMS = sizes.max()  # padding\n",
    "\n",
    "all_items_features = []\n",
    "all_choices = []\n",
    "all_available_items = []\n",
    "\n",
    "for srch_id, group in expedia_df.groupby(\"srch_id\"):\n",
    "    n = len(group)\n",
    "    chosen_idx = group[\"booking_bool\"].values.argmax()\n",
    "\n",
    "    # features shape\n",
    "    feat_matrix = np.zeros((MAX_ITEMS, len(item_features)))\n",
    "    feat_matrix[:n] = group[item_features].values\n",
    "\n",
    "    # availability mask\n",
    "    avail = np.zeros(MAX_ITEMS)\n",
    "    avail[:n] = 1\n",
    "\n",
    "    all_items_features.append(feat_matrix)\n",
    "    all_choices.append(chosen_idx)\n",
    "    all_available_items.append(avail)\n",
    "\n",
    "all_items_features = np.array(all_items_features)   # (n_searches, MAX_ITEMS, n_features)\n",
    "all_choices        = np.array(all_choices)           # (n_searches,)\n",
    "all_available_items = np.array(all_available_items) # (n_searches, MAX_ITEMS)\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {all_items_features.shape}\")\n",
    "print(f\"Choices shape: {all_choices.shape}\")\n",
    "\n",
    "# 80/20 split\n",
    "n_total    = len(all_choices)\n",
    "train_size = int(0.8 * n_total)\n",
    "\n",
    "X_train = all_items_features[:train_size]\n",
    "X_test  = all_items_features[train_size:]\n",
    "y_train = all_choices[:train_size]\n",
    "y_test  = all_choices[train_size:]\n",
    "avail_train = all_available_items[:train_size]\n",
    "avail_test  = all_available_items[train_size:]\n",
    "\n",
    "# Conditional Logit = linear utility + softmax\n",
    "\n",
    "class ConditionalLogitModel(tf.keras.Model):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.beta = tf.keras.layers.Dense(1, use_bias=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.squeeze(self.beta(inputs), axis=-1)\n",
    "\n",
    "def masked_crossentropy(y_true, logits, availability):\n",
    "    mask = (1 - availability) * (-1e9)\n",
    "    logits_masked = logits + mask\n",
    "    # cross entropy loss\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        y_true, logits_masked, from_logits=True)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# standardized features\n",
    "mean = X_train.mean(axis=(0,1), keepdims=True)\n",
    "std  = X_train.std(axis=(0,1), keepdims=True) + 1e-8\n",
    "X_train_scaled = (X_train - mean) / std\n",
    "X_test_scaled  = (X_test  - mean) / std\n",
    "\n",
    "# train\n",
    "cl_model   = ConditionalLogitModel(n_features=len(item_features))\n",
    "optimizer  = tf.keras.optimizers.Adam(0.01)\n",
    "\n",
    "for epoch in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = cl_model(X_train_scaled)\n",
    "        loss   = masked_crossentropy(y_train, logits, avail_train)\n",
    "    grads = tape.gradient(loss, cl_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, cl_model.trainable_variables))\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}/100 — loss: {loss:.4f}\")\n",
    "\n",
    "# test loss\n",
    "test_logits = cl_model(X_test_scaled)\n",
    "cl_loss = masked_crossentropy(y_test, test_logits, avail_test).numpy()\n",
    "print(f\"\\nConditional Logit — test cross-entropy: {cl_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1f7671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2c\n",
    "weights = cl_model.beta.get_weights()[0].flatten()\n",
    "coef_df = pd.DataFrame({\"feature\": item_features, \"coefficient\": weights})\n",
    "coef_df = coef_df.sort_values(\"coefficient\", ascending=False)\n",
    "print(\"\\nCoefficients:\")\n",
    "print(coef_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda90a3b",
   "metadata": {},
   "source": [
    "Question 2c\n",
    "Symbol expectation is intuitive. The most important characteristics are usually price and review_score, as these two are the most intuitive and easily comparable signals for users to make decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be830af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2d\n",
    "\n",
    "class RUMnetSimple(tf.keras.Model):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        # 两层MLP学习非线性效用函数\n",
    "        self.net = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(1)   # 输出是scalar utility\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.squeeze(self.net(inputs), axis=-1)\n",
    "\n",
    "rumnet_model = RUMnetSimple(n_features=len(item_features))\n",
    "optimizer2   = tf.keras.optimizers.Adam(0.01)\n",
    "\n",
    "for epoch in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = rumnet_model(X_train_scaled)\n",
    "        loss   = masked_crossentropy(y_train, logits, avail_train)\n",
    "    grads = tape.gradient(loss, rumnet_model.trainable_variables)\n",
    "    optimizer2.apply_gradients(zip(grads, rumnet_model.trainable_variables))\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}/100 — loss: {loss:.4f}\")\n",
    "\n",
    "test_logits2 = rumnet_model(X_test_scaled)\n",
    "rum_loss = masked_crossentropy(y_test, test_logits2, avail_test).numpy()\n",
    "\n",
    "print(f\"\\nRUMnet (MLP)      — test cross-entropy: {rum_loss:.4f}\")\n",
    "print(f\"Conditional Logit — test cross-entropy: {cl_loss:.4f}\")\n",
    "print(f\"Improvement: {cl_loss - rum_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d86f2c",
   "metadata": {},
   "source": [
    "Question 2e           \n",
    "RUMnet can capture nonlinearity/interaction with a large number of product features + customer features, which may theoretically be better on CE, but RUMnet needs better regularization/early stopping/hyperparameters. However, if the sample is small ( only use 5000 choices), the logit may be more stable. And for explanality, the logit is better to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3b8d25",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
